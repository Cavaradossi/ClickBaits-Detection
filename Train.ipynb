{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Loading JSON dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "from textblob import Word\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import nltk\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "def read_truth(path):\n",
    "    data = []\n",
    "    index = []\n",
    "    for i, line in enumerate(open(path + '/truth.jsonl', 'r')):\n",
    "        instance = json.loads(line)\n",
    "        data.append(instance)\n",
    "        index.append(instance['id'])\n",
    "    df = pd.DataFrame(data=data, index=index)\n",
    "    df.sort_index(inplace=True)\n",
    "    return df\n",
    "\n",
    "def read_instance(path):\n",
    "    data = []\n",
    "    index = []\n",
    "    for i, line in enumerate(open(path + '/instances.jsonl', 'rb')):\n",
    "        instance = json.loads(line)\n",
    "        data.append(instance)\n",
    "        index.append(instance['id'])\n",
    "    df = pd.DataFrame(data=data, index=index)\n",
    "    df.sort_index(inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "train_truth = read_truth('./clickbait17-train-170331')\n",
    "train_instances = read_instance('./clickbait17-train-170331')\n",
    "validation_truth = read_truth('./clickbait17-validation-170630')\n",
    "validation_instances = read_instance('./clickbait17-validation-170630')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.merge(train_truth, train_instances, on = 'id')\n",
    "validation = pd.merge(validation_truth, validation_instances, on = 'id')\n",
    "data = pd.concat([train, validation],ignore_index = True)\n",
    "data['truthClass'] = data['truthClass'].map({'clickbait':True ,'no-clickbait':False}).astype(bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Basic Feature Extraction\n",
    "\n",
    "## 2.1 number of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_word(sentence):\n",
    "    sentence = str(sentence)\n",
    "    words = sentence.split()\n",
    "    return round((sum(len(word) for word in words)/len(words)), 1)\n",
    "\n",
    "\n",
    "char_title = []\n",
    "word_title = []\n",
    "avg_paragraph = []\n",
    "char_paragraph = []\n",
    "word_paragraph = []\n",
    "char_caption = []\n",
    "word_caption = []\n",
    "\n",
    "for title in data['targetTitle']:\n",
    "    char = len(str(title))\n",
    "    word = len(str(title).split(' '))\n",
    "    char_title.append(char)\n",
    "    word_title.append(word)\n",
    "    \n",
    "for paragraph in data['targetParagraphs']:\n",
    "    char = len(str(paragraph))\n",
    "    word = len(str(paragraph).split(' '))\n",
    "    avg = avg_word(paragraph)\n",
    "    avg_paragraph.append(avg)\n",
    "    char_paragraph.append(char)\n",
    "    word_paragraph.append(word)\n",
    "    \n",
    "for caption in data['targetCaptions']:\n",
    "    char = len(str(caption))\n",
    "    word = len(str(caption).split(' '))\n",
    "    char_caption.append(char)\n",
    "    word_caption.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_feature = pd.DataFrame([char_caption,\n",
    "                             word_caption,\n",
    "                             char_title,\n",
    "                             word_title,\n",
    "                             char_paragraph,\n",
    "                             word_paragraph,\n",
    "                             avg_paragraph])\n",
    "char_feature = char_feature.T\n",
    "char_feature.rename(columns = {0: 'char_caption',\n",
    "                               1: 'word_caption',\n",
    "                               2: 'char_title',\n",
    "                               3: 'word_title',\n",
    "                               4: 'char_paragraph',\n",
    "                               5: 'word_paragraph',\n",
    "                               6: 'avg_paragraph',\n",
    "                              },inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>char_caption</th>\n",
       "      <th>word_caption</th>\n",
       "      <th>char_title</th>\n",
       "      <th>word_title</th>\n",
       "      <th>char_paragraph</th>\n",
       "      <th>word_paragraph</th>\n",
       "      <th>avg_paragraph</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>21997.000000</td>\n",
       "      <td>21997.000000</td>\n",
       "      <td>21997.000000</td>\n",
       "      <td>21997.000000</td>\n",
       "      <td>21997.000000</td>\n",
       "      <td>21997.000000</td>\n",
       "      <td>21997.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>585.354730</td>\n",
       "      <td>83.002182</td>\n",
       "      <td>80.443333</td>\n",
       "      <td>13.201755</td>\n",
       "      <td>3637.643406</td>\n",
       "      <td>594.734464</td>\n",
       "      <td>5.224603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2605.827071</td>\n",
       "      <td>329.696620</td>\n",
       "      <td>152.086847</td>\n",
       "      <td>23.953162</td>\n",
       "      <td>4226.469811</td>\n",
       "      <td>706.772198</td>\n",
       "      <td>1.348661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>39.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>1464.000000</td>\n",
       "      <td>237.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>142.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>2650.000000</td>\n",
       "      <td>429.000000</td>\n",
       "      <td>5.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>458.000000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>4643.000000</td>\n",
       "      <td>757.000000</td>\n",
       "      <td>5.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>144852.000000</td>\n",
       "      <td>16316.000000</td>\n",
       "      <td>4038.000000</td>\n",
       "      <td>651.000000</td>\n",
       "      <td>199672.000000</td>\n",
       "      <td>33306.000000</td>\n",
       "      <td>112.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        char_caption  word_caption    char_title    word_title  \\\n",
       "count   21997.000000  21997.000000  21997.000000  21997.000000   \n",
       "mean      585.354730     83.002182     80.443333     13.201755   \n",
       "std      2605.827071    329.696620    152.086847     23.953162   \n",
       "min         2.000000      1.000000      4.000000      1.000000   \n",
       "25%        39.000000      4.000000     56.000000      9.000000   \n",
       "50%       142.000000     20.000000     67.000000     11.000000   \n",
       "75%       458.000000     69.000000     80.000000     13.000000   \n",
       "max    144852.000000  16316.000000   4038.000000    651.000000   \n",
       "\n",
       "       char_paragraph  word_paragraph  avg_paragraph  \n",
       "count    21997.000000    21997.000000   21997.000000  \n",
       "mean      3637.643406      594.734464       5.224603  \n",
       "std       4226.469811      706.772198       1.348661  \n",
       "min          2.000000        1.000000       2.000000  \n",
       "25%       1464.000000      237.000000       5.000000  \n",
       "50%       2650.000000      429.000000       5.200000  \n",
       "75%       4643.000000      757.000000       5.400000  \n",
       "max     199672.000000    33306.000000     112.000000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_feature.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Text feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_at = []\n",
    "num_acronym = []\n",
    "num_baity = []#!\n",
    "num_cap = []\n",
    "num_digit = []\n",
    "num_exclm = []\n",
    "num_money = []\n",
    "num_ques = []\n",
    "num_quote = []#!\n",
    "num_tag = []\n",
    "num_pic = []\n",
    "num_parenthesis = []\n",
    "is_start_num = []\n",
    "is_superlative = []\n",
    "is_start5w1h = []\n",
    "\n",
    "table_currency = ['¥','$','€','£','￠']\n",
    "table_bracket = ['(',')','[',']','{','}']\n",
    "table_quote = [\"'m\",\"'re\",\"'ve\",\"'d\",\"'s\",\"s'\"]\n",
    "table_baity = [\"click here\",\"exclusive\",\"won't believe\",\"happen next\",\"don't want\",\"you know\"]\n",
    "table_5w1h = [\"what\",\"why\",\"when\",\"who\",\"which\",\"how\"]\n",
    "\n",
    "for caption in data['targetCaptions']:\n",
    "    num_pic.append(len(set(caption)))  \n",
    "\n",
    "for sentences in data['targetTitle']:\n",
    "    sentences = str(sentences)\n",
    "    tmp = sentences.split()\n",
    "    s = str(tmp[0])\n",
    "    \n",
    "    question_mark = 0\n",
    "    start_digit = False\n",
    "    start_5w1h = False\n",
    "    parenthesis = 0\n",
    "    superlative = False\n",
    "    exclamation = 0\n",
    "    digital = 0\n",
    "    acronym = 0\n",
    "    capital = 0\n",
    "    money = 0\n",
    "    baity = 0\n",
    "    quote = 0\n",
    "    start = 0\n",
    "    hash_tag = 0\n",
    "    at = 0\n",
    "    \n",
    "    text = nltk.word_tokenize(sentences)\n",
    "    part_of_speech = nltk.pos_tag(text)\n",
    "    for token,tag in part_of_speech:\n",
    "        if tag in ['RBS','JJS']:\n",
    "            superlative  = True\n",
    "            \n",
    "    if s in table_5w1h:\n",
    "        start_5w1h = True\n",
    "    if s.isdigit():\n",
    "        start_digit = True\n",
    "        \n",
    "    for word in sentences.split():        \n",
    "        if len(word) <= 5:\n",
    "            acronym += 1\n",
    "        if len(word) > 5:\n",
    "            if word.isupper():\n",
    "                capital += 1                \n",
    "        for char in word:\n",
    "            if char == '!':\n",
    "                exclamation += 1\n",
    "            if char == '?':\n",
    "                question_mark += 1\n",
    "            if char in table_currency:\n",
    "                money += 1\n",
    "            if char in table_bracket:\n",
    "                parenthesis += 1\n",
    "            if char == '@':\n",
    "                at += 1\n",
    "            if char == '#':\n",
    "                hash_tag += 1\n",
    "            if char.isdigit:\n",
    "                digital += 1\n",
    "    \n",
    "    num_acronym.append(acronym)\n",
    "    num_at.append(at)\n",
    "    num_cap.append(capital)\n",
    "    num_digit.append(digital)\n",
    "    num_exclm.append(exclamation)\n",
    "    num_money.append(money)\n",
    "    num_parenthesis.append(parenthesis)\n",
    "    num_ques.append(question_mark)\n",
    "    num_tag.append(hash_tag)\n",
    "    is_start5w1h.append(start_5w1h)\n",
    "    is_start_num.append(start_digit)\n",
    "    is_superlative.append(superlative)\n",
    "#     num_baity.append(baity)\n",
    "#     num_quote.append(quote)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_feature = pd.DataFrame([num_acronym,\n",
    "                             num_at,\n",
    "                             num_cap,\n",
    "                             num_digit,\n",
    "                             num_exclm,\n",
    "                             num_money,\n",
    "                             num_parenthesis,\n",
    "                             num_ques,\n",
    "                             num_tag,\n",
    "                             num_pic,\n",
    "                             is_start_num, \n",
    "                             is_superlative,\n",
    "#                              is_start5w1h\n",
    "                               ])\n",
    "title_feature = title_feature.T\n",
    "title_feature.rename(columns = {0:'num_acronym',\n",
    "                               1:'num_at',\n",
    "                               2:'num_cap',\n",
    "                               3:'num_digit',\n",
    "                               4:'num_exclm',\n",
    "                               5:'num_money',\n",
    "                               6:'num_parenthesis',\n",
    "                               7:'num_ques',\n",
    "                               8:'num_tag',\n",
    "                               9:'num_pic',\n",
    "                               10:'is_start_num',  \n",
    "                               11:'is_superlative',\n",
    "#                                10:'is_start5w1h'\n",
    "                              },inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_acronym</th>\n",
       "      <th>num_at</th>\n",
       "      <th>num_cap</th>\n",
       "      <th>num_digit</th>\n",
       "      <th>num_exclm</th>\n",
       "      <th>num_money</th>\n",
       "      <th>num_parenthesis</th>\n",
       "      <th>num_ques</th>\n",
       "      <th>num_tag</th>\n",
       "      <th>num_pic</th>\n",
       "      <th>is_start_num</th>\n",
       "      <th>is_superlative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>21997</td>\n",
       "      <td>21997</td>\n",
       "      <td>21997</td>\n",
       "      <td>21997</td>\n",
       "      <td>21997</td>\n",
       "      <td>21997</td>\n",
       "      <td>21997</td>\n",
       "      <td>21997</td>\n",
       "      <td>21997</td>\n",
       "      <td>21997</td>\n",
       "      <td>21997</td>\n",
       "      <td>21997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>81</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>264</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>98</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>53</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>2996</td>\n",
       "      <td>21988</td>\n",
       "      <td>21599</td>\n",
       "      <td>656</td>\n",
       "      <td>21513</td>\n",
       "      <td>21398</td>\n",
       "      <td>21688</td>\n",
       "      <td>20897</td>\n",
       "      <td>21941</td>\n",
       "      <td>6495</td>\n",
       "      <td>21095</td>\n",
       "      <td>21009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        num_acronym  num_at  num_cap  num_digit  num_exclm  num_money  \\\n",
       "count         21997   21997    21997      21997      21997      21997   \n",
       "unique           81       2        4        264          4          5   \n",
       "top               6       0        0         53          0          0   \n",
       "freq           2996   21988    21599        656      21513      21398   \n",
       "\n",
       "        num_parenthesis  num_ques  num_tag  num_pic is_start_num  \\\n",
       "count             21997     21997    21997    21997        21997   \n",
       "unique                3         7        3       98            2   \n",
       "top                   0         0        0        1        False   \n",
       "freq              21688     20897    21941     6495        21095   \n",
       "\n",
       "       is_superlative  \n",
       "count           21997  \n",
       "unique              2  \n",
       "top             False  \n",
       "freq            21009  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_feature.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Informality and Forward Reference features\n",
    "+ CLScore = 0.0588*L - 0.296*S - 15.8  \n",
    "\n",
    "   L = average number of letters     \n",
    "   S = average number of sentence per 100 words  \n",
    "   \n",
    "\n",
    "+ RIX = LW/S  \n",
    "\n",
    "+ LIX = W/S + (100*LW)/W  \n",
    "\n",
    "   W = number of words     \n",
    "   LW number of long words(7+ characters)     \n",
    "   S = number of sentence  \n",
    "   \n",
    "   \n",
    "+ Formality Measure (fmeasure):  \n",
    "\n",
    "   (nounfreq+adjectivefreq+prepositionfreq+particlefreq-pronounfreq-verbfreq-adverbfreq-interjectionfreq+100)*0.5   \n",
    "   \n",
    "\n",
    "+ Sentiment Analysis:  \n",
    "\n",
    "   extract polarity as it indicates the sentiment as value nearer to 1 means a positive sentiment and values nearer to -1 means a negative sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "RIX = []\n",
    "LIX = []\n",
    "sentiment = []\n",
    "CLScore = []\n",
    "f_measure = []\n",
    "\n",
    "for paragraph in data['targetParagraphs']:\n",
    "    \n",
    "    avg_letter = 0\n",
    "    avg_sentence = 0\n",
    "    num_word = 0\n",
    "    num_char = 0\n",
    "    num_sentence = 0\n",
    "    long_word = 0\n",
    "    noun_freq = 0\n",
    "    adjective_freq = 0\n",
    "    preposition_freq = 0\n",
    "    particle_freq = 0\n",
    "    pronoun_freq = 0\n",
    "    verb_freq = 0\n",
    "    adverb_freq = 0\n",
    "    interjection_freq = 0  \n",
    "    measure = 0\n",
    "    \n",
    "    paragraph = str(paragraph)\n",
    "    text = nltk.word_tokenize(paragraph)\n",
    "    part_of_speech = nltk.pos_tag(text)\n",
    "    for token,tag in part_of_speech:\n",
    "        if tag in ['NN','NNS','NNP','NNPS']:\n",
    "            noun_freq += 1\n",
    "        if tag in ['VB','VBD','VBG','VBN','VBP','VBZ']:\n",
    "            verb_freq += 1\n",
    "        if tag == 'UH':\n",
    "            interjection_freq += 1\n",
    "        if tag in ['RB','RBS','RBR']:\n",
    "            adverb_freq += 1\n",
    "        if tag == 'RP':\n",
    "            particle_freq += 1\n",
    "        if tag in ['JJ','JJR','JJS'] :\n",
    "            adjective_freq += 1\n",
    "        if tag == 'in':\n",
    "            preposition_freq += 1\n",
    "        if tag in ['WRB','WP$','WP','PRP$','PRP']:\n",
    "            pronoun_freq += 1\n",
    "    measure = (noun_freq+adjective_freq+preposition_freq+particle_freq-pronoun_freq-verb_freq-adverb_freq-interjection_freq)\n",
    "        \n",
    "    for word in paragraph.split():\n",
    "        num_word += 1\n",
    "        num_char += len(word)\n",
    "        if len(word) >= 7:\n",
    "            long_word += 1\n",
    "        for char in word:\n",
    "            if char == '.':\n",
    "                num_sentence += 1\n",
    "    \n",
    "    h_word = num_word//100\n",
    "    if h_word == 0:\n",
    "        h_word = 1\n",
    "    if num_sentence == 0:\n",
    "        num_sentence = 1\n",
    "    if num_word == 0:\n",
    "        num_word = 1\n",
    "\n",
    "    avg_letter = round((num_char / num_word), 1)\n",
    "    avg_sentence = round((num_sentence / h_word), 1)\n",
    "    RIX.append(round(long_word/num_sentence, 1))\n",
    "    CLScore.append(round(0.0588*avg_letter-0.296*avg_sentence-15.8, 1))\n",
    "    sentiment.append(round(TextBlob(paragraph).sentiment[0], 1))\n",
    "    LIX.append(round(num_word/num_sentence + (100*long_word)/num_word, 1))\n",
    "    f_measure.append((measure+100)/2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_feature = pd.DataFrame([RIX,\n",
    "                             LIX,\n",
    "                             sentiment,\n",
    "                             CLScore,\n",
    "                             f_measure,\n",
    "                               ])\n",
    "content_feature = content_feature.T\n",
    "content_feature.rename(columns = {0:'RIX',\n",
    "                               1:'LIX',\n",
    "                               2:'sentiment',\n",
    "                               3:'CLScore',\n",
    "                               4:'f_measure',\n",
    "                              },inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RIX</th>\n",
       "      <th>LIX</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>CLScore</th>\n",
       "      <th>f_measure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>21997.000000</td>\n",
       "      <td>21997.000000</td>\n",
       "      <td>21997.000000</td>\n",
       "      <td>21997.000000</td>\n",
       "      <td>21997.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.565254</td>\n",
       "      <td>47.946152</td>\n",
       "      <td>0.095872</td>\n",
       "      <td>-17.396790</td>\n",
       "      <td>96.145906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.774795</td>\n",
       "      <td>12.432667</td>\n",
       "      <td>0.110498</td>\n",
       "      <td>0.930461</td>\n",
       "      <td>69.958521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-35.200000</td>\n",
       "      <td>-521.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>42.300000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-17.700000</td>\n",
       "      <td>63.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.200000</td>\n",
       "      <td>47.500000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>-17.300000</td>\n",
       "      <td>81.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.500000</td>\n",
       "      <td>52.500000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>-16.900000</td>\n",
       "      <td>111.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>142.000000</td>\n",
       "      <td>515.200000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-10.100000</td>\n",
       "      <td>3373.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                RIX           LIX     sentiment       CLScore     f_measure\n",
       "count  21997.000000  21997.000000  21997.000000  21997.000000  21997.000000\n",
       "mean       5.565254     47.946152      0.095872    -17.396790     96.145906\n",
       "std        3.774795     12.432667      0.110498      0.930461     69.958521\n",
       "min        0.000000      1.000000     -1.000000    -35.200000   -521.500000\n",
       "25%        4.000000     42.300000      0.000000    -17.700000     63.000000\n",
       "50%        5.200000     47.500000      0.100000    -17.300000     81.500000\n",
       "75%        6.500000     52.500000      0.100000    -16.900000    111.000000\n",
       "max      142.000000    515.200000      1.000000    -10.100000   3373.500000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_feature.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Similarity between Title and Top 5 Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "top1sim = []\n",
    "top2sim = []\n",
    "top3sim = []\n",
    "top4sim = []\n",
    "top5sim = []\n",
    "\n",
    "for i in range(len(data)):\n",
    "    \n",
    "    title = data['targetTitle'][i]\n",
    "    body = data['targetParagraphs'][i]\n",
    "    sim_1 = sim_2 = sim_3 = sim_4 = sim_5 = 0.0\n",
    "    sen_1 = sen_2 = sen_3 = sen_4 = sen_5 = ''\n",
    "    \n",
    "    if len(body) == 0:\n",
    "        top1sim.append(sim_1)\n",
    "        top2sim.append(sim_2)\n",
    "        top3sim.append(sim_3)\n",
    "        top4sim.append(sim_4)\n",
    "        top5sim.append(sim_5)\n",
    "        continue\n",
    "    if len(body) >= 1:\n",
    "        sen_1 = body[0]\n",
    "    if len(body) >= 2:\n",
    "        sen_2 = body[1]\n",
    "    if len(body) >= 3:\n",
    "        sen_3 = body[2]\n",
    "    if len(body) >= 4:\n",
    "        sen_4 = body[3]\n",
    "    if len(body) >= 5:\n",
    "        sen_5 = body[4]\n",
    "    \n",
    "    essay = str(title + ' ' + sen_1 + ' ' + sen_2 + ' '  + sen_3 + ' ' + sen_4 + ' ' + sen_5)\n",
    "    tfidf = TfidfVectorizer(lowercase = True, analyzer = 'word', stop_words = 'english',ngram_range = (1,1))\n",
    "    train_vect = tfidf.fit_transform([essay])    \n",
    "    title_vect = tfidf.transform([title])\n",
    "    \n",
    "    s1_vect = tfidf.transform([sen_1])\n",
    "    sim_1 = round(float(cosine_similarity(title_vect,s1_vect)),1)\n",
    "    top1sim.append(sim_1)\n",
    "    \n",
    "    s2_vect = tfidf.transform([sen_2])\n",
    "    sim_2 = round(float(cosine_similarity(title_vect,s2_vect)),1)\n",
    "    top2sim.append(sim_2)\n",
    "    \n",
    "    s3_vect = tfidf.transform([sen_3])\n",
    "    sim_3 = round(float(cosine_similarity(title_vect,s3_vect)),1)\n",
    "    top3sim.append(sim_3)\n",
    "    \n",
    "    s4_vect = tfidf.transform([sen_4])\n",
    "    sim_4 = round(float(cosine_similarity(title_vect,s4_vect)),1)\n",
    "    top4sim.append(sim_4)\n",
    "    \n",
    "    s5_vect = tfidf.transform([sen_5])\n",
    "    sim_5 = round(float(cosine_similarity(title_vect,s5_vect)),1)\n",
    "    top5sim.append(sim_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_feature = pd.DataFrame([top1sim,\n",
    "                             top2sim,\n",
    "                             top3sim,\n",
    "                             top4sim,\n",
    "                             top5sim,\n",
    "                               ])\n",
    "sim_feature = sim_feature.T\n",
    "sim_feature.rename(columns = {0:'top1sim',\n",
    "                               1:'top2sim',\n",
    "                               2:'top3sim',\n",
    "                               3:'top4sim',\n",
    "                               4:'top5sim',\n",
    "                              },inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>top1sim</th>\n",
       "      <th>top2sim</th>\n",
       "      <th>top3sim</th>\n",
       "      <th>top4sim</th>\n",
       "      <th>top5sim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>21997.000000</td>\n",
       "      <td>21997.000000</td>\n",
       "      <td>21997.000000</td>\n",
       "      <td>21997.00000</td>\n",
       "      <td>21997.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.237628</td>\n",
       "      <td>0.166332</td>\n",
       "      <td>0.126017</td>\n",
       "      <td>0.11778</td>\n",
       "      <td>0.100714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.208294</td>\n",
       "      <td>0.166482</td>\n",
       "      <td>0.139019</td>\n",
       "      <td>0.13668</td>\n",
       "      <td>0.122919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.10000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.20000</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            top1sim       top2sim       top3sim      top4sim       top5sim\n",
       "count  21997.000000  21997.000000  21997.000000  21997.00000  21997.000000\n",
       "mean       0.237628      0.166332      0.126017      0.11778      0.100714\n",
       "std        0.208294      0.166482      0.139019      0.13668      0.122919\n",
       "min        0.000000      0.000000      0.000000      0.00000      0.000000\n",
       "25%        0.000000      0.000000      0.000000      0.00000      0.000000\n",
       "50%        0.200000      0.100000      0.100000      0.10000      0.100000\n",
       "75%        0.400000      0.300000      0.200000      0.20000      0.200000\n",
       "max        1.000000      1.000000      1.000000      1.00000      1.000000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_feature.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Basic Pre-processing\n",
    "\n",
    "## 3.1 Lower case Removing Punctuation Removal of Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_corpus = stopwords.words('english')\n",
    "stop_words = []\n",
    "caption = []\n",
    "content = []\n",
    "title = []\n",
    "\n",
    "for sentence in data['targetParagraphs']:\n",
    "    sentence = str(sentence).replace('[^\\w\\s]','')\n",
    "    sentence = re.sub('[\\u0060|\\u0021-\\u002c|\\u002e-\\u002f|\\u003a-\\u003f|\\u2200-\\u22ff|\\uFB00-\\uFFFD|\\u2E80-\\u33FF]',' ',sentence)\n",
    "    stop_cnt = 0\n",
    "    string = ''\n",
    "#     sentence = TextBlob(sentence).correct()\n",
    "    for word in sentence.split():\n",
    "        if word in en_corpus:\n",
    "            stop_cnt += 1\n",
    "        if word not in en_corpus:            \n",
    "            string += ' ' + word.lower()\n",
    "    stop_words.append(stop_cnt)\n",
    "    content.append(string)    \n",
    "\n",
    "for sentence in data['targetCaptions']:\n",
    "    string = ''\n",
    "    sentence = str(sentence).replace('[^\\w\\s]','')\n",
    "    sentence = re.sub('[\\u0060|\\u0021-\\u002c|\\u002e-\\u002f|\\u003a-\\u003f|\\u2200-\\u22ff|\\uFB00-\\uFFFD|\\u2E80-\\u33FF]',' ',sentence)\n",
    "    for word in sentence.split():\n",
    "        if word not in en_corpus:            \n",
    "            string += ' ' + word.lower()\n",
    "    caption.append(string) \n",
    "\n",
    "for sentence in data['targetTitle']:\n",
    "    string = ''\n",
    "    sentence = str(sentence).replace('[^\\w\\s]','')\n",
    "    sentence = re.sub('[\\u0060|\\u0021-\\u002c|\\u002e-\\u002f|\\u003a-\\u003f|\\u2200-\\u22ff|\\uFB00-\\uFFFD|\\u2E80-\\u33FF]',' ',sentence)\n",
    "    for word in sentence.split():\n",
    "        if word not in en_corpus:            \n",
    "            string += ' ' + word.lower()\n",
    "    title.append(string)\n",
    "    \n",
    "corpus = title + content + caption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Common / Rare words removal & Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_caption = []\n",
    "target_content = []\n",
    "target_title = []\n",
    "\n",
    "common = pd.Series(' '.join(caption).split()).value_counts()[:10]\n",
    "rare = pd.Series(' '.join(caption).split()).value_counts()[-10:]  \n",
    "\n",
    "for sentence in caption:\n",
    "    string = ''\n",
    "    lemma = ''\n",
    "    sentence = str(sentence)\n",
    "    for word in sentence.split():\n",
    "        if word not in common:\n",
    "            if word  not in rare:\n",
    "                lemma = Word(word).lemmatize()\n",
    "                string += ' ' + lemma\n",
    "    target_caption.append(string)\n",
    "\n",
    "common = pd.Series(' '.join(content).split()).value_counts()[:10]\n",
    "rare = pd.Series(' '.join(content).split()).value_counts()[-10:]  \n",
    "\n",
    "for sentence in content:\n",
    "    string = ''\n",
    "    sentence = str(sentence)\n",
    "    for word in sentence.split():\n",
    "        if word not in common:\n",
    "            if word  not in rare:\n",
    "                lemma = Word(word).lemmatize()\n",
    "                string += ' ' + lemma\n",
    "    target_content.append(string)\n",
    "    \n",
    "common = pd.Series(' '.join(title).split()).value_counts()[:10]\n",
    "rare = pd.Series(' '.join(title).split()).value_counts()[-10:]  \n",
    "\n",
    "for sentence in title:\n",
    "    string = ''\n",
    "    sentence = str(sentence)\n",
    "    for word in sentence.split():\n",
    "        if word not in common:\n",
    "            if word  not in rare:\n",
    "                lemma = Word(word).lemmatize()\n",
    "                string += ' ' + lemma\n",
    "    target_title.append(string) \\\n",
    "    \n",
    "target_corpus = []\n",
    "for i in range(len(target_content)):\n",
    "    total_context = str(target_caption[i] + target_content[i] + target_title[i])\n",
    "    target_corpus.append(total_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.Advance Text Processing\n",
    "## 4.1 N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WordList(['tony', 'nominees']),\n",
       " WordList(['nominees', 'craziest']),\n",
       " WordList(['craziest', 'moments']),\n",
       " WordList(['moments', 'stage'])]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextBlob(corpus[0]).ngrams(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(max_features = 1024, lowercase = True, analyzer = 'word', stop_words = 'english',ngram_range = (1,1))\n",
    "corpus_vect = tfidf.fit_transform(target_corpus)\n",
    "\n",
    "vec_feature = []\n",
    "\n",
    "for article in target_corpus:\n",
    "    corpus_vect = tfidf.transform([article])\n",
    "    vec_feature.append(corpus_vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<21997x1000 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 2535921 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow = CountVectorizer(max_features=1000, lowercase=True, ngram_range=(1,1),analyzer = \"word\")\n",
    "train_bow = bow.fit_transform(target_corpus)\n",
    "\n",
    "train_bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_input_file = 'glove.840B.300d.txt'\n",
    "word2vec_output_file = 'glove.840B.300d.txt.word2vec'\n",
    "glove2word2vec(glove_input_file, word2vec_output_file)\n",
    "filename = 'glove.840B.300d.txt.word2vec'\n",
    "model = KeyedVectors.load_word2vec_format(filename, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'glove.840B.300d.txt.word2vec'\n",
    "model = KeyedVectors.load_word2vec_format(filename, binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'[ 8027 17745 15991 ...  1281 14078 20125] not in index'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-b75a7b8fced9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtrain_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_index\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m     \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m     \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2680\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2681\u001b[0m             \u001b[1;31m# either boolean or fancy integer index\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2682\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2683\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2684\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_getitem_array\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2724\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_take\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2725\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2726\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_convert_to_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2727\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_take\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2728\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_convert_to_indexer\u001b[1;34m(self, obj, axis, is_setter)\u001b[0m\n\u001b[0;32m   1325\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1326\u001b[0m                     raise KeyError('{mask} not in index'\n\u001b[1;32m-> 1327\u001b[1;33m                                    .format(mask=objarr[mask]))\n\u001b[0m\u001b[0;32m   1328\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1329\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values_from_object\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: '[ 8027 17745 15991 ...  1281 14078 20125] not in index'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(3),\n",
    "    SVC(probability=True),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    AdaBoostClassifier(),\n",
    "    GradientBoostingClassifier(),\n",
    "    GaussianNB(),\n",
    "    LinearDiscriminantAnalysis(),\n",
    "    QuadraticDiscriminantAnalysis(),\n",
    "    LogisticRegression()]\n",
    "classifiers_name = [\n",
    "    \"KNeighborsClassifier\",\n",
    "    \"SVC\",\n",
    "   \" DecisionTreeClassifier\",\n",
    "    \"RandomForestClassifier\",\n",
    "    \"AdaBoostClassifier\",\n",
    "    \"GradientBoostingClassifier\",\n",
    "    \"GaussianNB\",\n",
    "    \"LinearDiscriminantAnalysis\",\n",
    "    \"QuadraticDiscriminantAnalysis\",\n",
    "    \"LogisticRegression\"]\n",
    "\n",
    "log_cols = ['Classifier','Accuracy']\n",
    "log = pd.DataFrame(columns = log_cols)\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits = 10, test_size = 0.1, random_state = 0)\n",
    "\n",
    "acc_dict = []\n",
    "\n",
    "handy_feature = pd.concat([char_feature,title_feature,content_feature,sim_feature], axis = 1)\n",
    "X = handy_feature\n",
    "y = data['truthMean']\n",
    "\n",
    "for train_index, test_index in sss.split(X,y):\n",
    "    X_train, X_test = X[train_index],y[test_index]\n",
    "    y_train, y_test = y[train_index],y[test_index]\n",
    "    \n",
    "    for clf in classsifiers:\n",
    "        name = clf.__class__.name__\n",
    "        clf.fit(X_train,y_train)\n",
    "        acc = accuracy_score(y_test,train_predictions)\n",
    "        if name in acc_dict:\n",
    "            acc_dict[name] += acc\n",
    "        else:\n",
    "            acc_dict[name] = acc\n",
    "        \n",
    "for clf in acc_dict:\n",
    "    acc_dict[clf] = acc_dict\n",
    "    log_entry = pd.DataFrame([[clf,acc_dict[clf]]],columns = log_cols)\n",
    "    \n",
    "print(log)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
