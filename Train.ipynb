{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Loading JSON dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "from textblob import Word\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import math\n",
    "import nltk\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "def read_truth(path):\n",
    "    data = []\n",
    "    index = []\n",
    "    for i, line in enumerate(open(path + '/truth.jsonl', 'r')):\n",
    "        instance = json.loads(line)\n",
    "        data.append(instance)\n",
    "        index.append(instance['id'])\n",
    "    df = pd.DataFrame(data=data, index=index)\n",
    "    df.sort_index(inplace=True)\n",
    "    return df\n",
    "\n",
    "def read_instance(path):\n",
    "    data = []\n",
    "    index = []\n",
    "    for i, line in enumerate(open(path + '/instances.jsonl', 'rb')):\n",
    "        instance = json.loads(line)\n",
    "        data.append(instance)\n",
    "        index.append(instance['id'])\n",
    "    df = pd.DataFrame(data=data, index=index)\n",
    "    df.sort_index(inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "train_truth = read_truth('./clickbait17-train-170331')\n",
    "train_instances = read_instance('./clickbait17-train-170331')\n",
    "validation_truth = read_truth('./clickbait17-validation-170630')\n",
    "validation_instances = read_instance('./clickbait17-validation-170630')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.merge(train_truth, train_instances, on = 'id')\n",
    "validation = pd.merge(validation_truth, validation_instances, on = 'id')\n",
    "data = pd.concat([train, validation],ignore_index = True)\n",
    "data['truthClass'] = data['truthClass'].map({'clickbait':True ,'no-clickbait':False}).astype(bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Basic Feature Extraction\n",
    "\n",
    "## 2.1 number of words\n",
    "\n",
    "+ Number of Words:  \n",
    "\n",
    "   title    \n",
    "   body    \n",
    "   caption \n",
    "   \n",
    "\n",
    "+ Number of character: \n",
    "\n",
    "   title    \n",
    "   body    \n",
    "   caption \n",
    "   \n",
    "   \n",
    "+ average word per sentence: \n",
    "\n",
    "   body "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_word(sentence):\n",
    "    sentence = str(sentence)\n",
    "    words = sentence.split()\n",
    "    return round((sum(len(word) for word in words)/len(words)), 1)\n",
    "\n",
    "\n",
    "char_title = []\n",
    "word_title = []\n",
    "avg_paragraph = []\n",
    "char_paragraph = []\n",
    "word_paragraph = []\n",
    "char_caption = []\n",
    "word_caption = []\n",
    "\n",
    "for title in data['targetTitle']:\n",
    "    char = len(str(title))\n",
    "    word = len(str(title).split(' '))\n",
    "    char_title.append(char)\n",
    "    word_title.append(word)\n",
    "    \n",
    "for paragraph in data['targetParagraphs']:\n",
    "    char = len(str(paragraph))\n",
    "    word = len(str(paragraph).split(' '))\n",
    "    avg = avg_word(paragraph)\n",
    "    avg_paragraph.append(avg)\n",
    "    char_paragraph.append(char)\n",
    "    word_paragraph.append(word)\n",
    "    \n",
    "for caption in data['targetCaptions']:\n",
    "    char = len(str(caption))\n",
    "    word = len(str(caption).split(' '))\n",
    "    char_caption.append(char)\n",
    "    word_caption.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_feature = pd.DataFrame([char_caption,\n",
    "                             word_caption,\n",
    "                             char_title,\n",
    "                             word_title,\n",
    "                             char_paragraph,\n",
    "                             word_paragraph,\n",
    "                             avg_paragraph])\n",
    "char_feature = char_feature.T\n",
    "char_feature.rename(columns = {0: 'char_caption',\n",
    "                               1: 'word_caption',\n",
    "                               2: 'char_title',\n",
    "                               3: 'word_title',\n",
    "                               4: 'char_paragraph',\n",
    "                               5: 'word_paragraph',\n",
    "                               6: 'avg_paragraph',\n",
    "                              },inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>char_caption</th>\n",
       "      <th>word_caption</th>\n",
       "      <th>char_title</th>\n",
       "      <th>word_title</th>\n",
       "      <th>char_paragraph</th>\n",
       "      <th>word_paragraph</th>\n",
       "      <th>avg_paragraph</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>21997.000000</td>\n",
       "      <td>21997.000000</td>\n",
       "      <td>21997.000000</td>\n",
       "      <td>21997.000000</td>\n",
       "      <td>21997.000000</td>\n",
       "      <td>21997.000000</td>\n",
       "      <td>21997.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>585.354730</td>\n",
       "      <td>83.002182</td>\n",
       "      <td>80.443333</td>\n",
       "      <td>13.201755</td>\n",
       "      <td>3637.643406</td>\n",
       "      <td>594.734464</td>\n",
       "      <td>5.224603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2605.827071</td>\n",
       "      <td>329.696620</td>\n",
       "      <td>152.086847</td>\n",
       "      <td>23.953162</td>\n",
       "      <td>4226.469811</td>\n",
       "      <td>706.772198</td>\n",
       "      <td>1.348661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>39.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>1464.000000</td>\n",
       "      <td>237.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>142.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>2650.000000</td>\n",
       "      <td>429.000000</td>\n",
       "      <td>5.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>458.000000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>4643.000000</td>\n",
       "      <td>757.000000</td>\n",
       "      <td>5.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>144852.000000</td>\n",
       "      <td>16316.000000</td>\n",
       "      <td>4038.000000</td>\n",
       "      <td>651.000000</td>\n",
       "      <td>199672.000000</td>\n",
       "      <td>33306.000000</td>\n",
       "      <td>112.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        char_caption  word_caption    char_title    word_title  \\\n",
       "count   21997.000000  21997.000000  21997.000000  21997.000000   \n",
       "mean      585.354730     83.002182     80.443333     13.201755   \n",
       "std      2605.827071    329.696620    152.086847     23.953162   \n",
       "min         2.000000      1.000000      4.000000      1.000000   \n",
       "25%        39.000000      4.000000     56.000000      9.000000   \n",
       "50%       142.000000     20.000000     67.000000     11.000000   \n",
       "75%       458.000000     69.000000     80.000000     13.000000   \n",
       "max    144852.000000  16316.000000   4038.000000    651.000000   \n",
       "\n",
       "       char_paragraph  word_paragraph  avg_paragraph  \n",
       "count    21997.000000    21997.000000   21997.000000  \n",
       "mean      3637.643406      594.734464       5.224603  \n",
       "std       4226.469811      706.772198       1.348661  \n",
       "min          2.000000        1.000000       2.000000  \n",
       "25%       1464.000000      237.000000       5.000000  \n",
       "50%       2650.000000      429.000000       5.200000  \n",
       "75%       4643.000000      757.000000       5.400000  \n",
       "max     199672.000000    33306.000000     112.000000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_feature.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Text feature \n",
    "\n",
    "counting certain character in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_at = []\n",
    "num_acronym = []\n",
    "num_baity = []#!\n",
    "num_cap = []\n",
    "num_digit = []\n",
    "num_exclm = []\n",
    "num_money = []\n",
    "num_ques = []\n",
    "num_quote = []#!\n",
    "num_tag = []\n",
    "num_pic = []\n",
    "num_parenthesis = []\n",
    "is_start_num = []\n",
    "is_superlative = []\n",
    "is_start5w1h = []\n",
    "\n",
    "table_currency = ['¥','$','€','£','￠']\n",
    "table_bracket = ['(',')','[',']','{','}']\n",
    "table_quote = [\"'m\",\"'re\",\"'ve\",\"'d\",\"'s\",\"s'\"]\n",
    "table_baity = [\"click here\",\"exclusive\",\"won't believe\",\"happen next\",\"don't want\",\"you know\"]\n",
    "table_5w1h = [\"what\",\"why\",\"when\",\"who\",\"which\",\"how\"]\n",
    "\n",
    "for caption in data['targetCaptions']:\n",
    "    num_pic.append(len(set(caption)))  \n",
    "\n",
    "for sentences in data['targetTitle']:\n",
    "    sentences = str(sentences)\n",
    "    tmp = sentences.split()\n",
    "    s = str(tmp[0])\n",
    "    \n",
    "    question_mark = 0\n",
    "    start_digit = -1\n",
    "    start_5w1h = -1\n",
    "    parenthesis = 0\n",
    "    superlative = -1\n",
    "    exclamation = 0\n",
    "    digital = 0\n",
    "    acronym = 0\n",
    "    capital = 0\n",
    "    money = 0\n",
    "    baity = 0\n",
    "    quote = 0\n",
    "    start = 0\n",
    "    hash_tag = 0\n",
    "    at = 0\n",
    "    \n",
    "    text = nltk.word_tokenize(sentences)\n",
    "    part_of_speech = nltk.pos_tag(text)\n",
    "    for token,tag in part_of_speech:\n",
    "        if tag in ['RBS','JJS']:\n",
    "            superlative  = 1\n",
    "            \n",
    "    if s in table_5w1h:\n",
    "        start_5w1h = 1\n",
    "    if s.isdigit():\n",
    "        start_digit = 1\n",
    "        \n",
    "    for word in sentences.split():        \n",
    "        if len(word) <= 5:\n",
    "            acronym += 1\n",
    "        if len(word) > 5:\n",
    "            if word.isupper():\n",
    "                capital += 1                \n",
    "        for char in word:\n",
    "            if char == '!':\n",
    "                exclamation += 1\n",
    "            if char == '?':\n",
    "                question_mark += 1\n",
    "            if char in table_currency:\n",
    "                money += 1\n",
    "            if char in table_bracket:\n",
    "                parenthesis += 1\n",
    "            if char == '@':\n",
    "                at += 1\n",
    "            if char == '#':\n",
    "                hash_tag += 1\n",
    "            if char.isdigit:\n",
    "                digital += 1\n",
    "    \n",
    "    num_acronym.append(acronym)\n",
    "    num_at.append(at)\n",
    "    num_cap.append(capital)\n",
    "    num_digit.append(digital)\n",
    "    num_exclm.append(exclamation)\n",
    "    num_money.append(money)\n",
    "    num_parenthesis.append(parenthesis)\n",
    "    num_ques.append(question_mark)\n",
    "    num_tag.append(hash_tag)\n",
    "    is_start5w1h.append(start_5w1h)\n",
    "    is_start_num.append(start_digit)\n",
    "    is_superlative.append(superlative)\n",
    "#     num_baity.append(baity)\n",
    "#     num_quote.append(quote)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_feature = pd.DataFrame([num_acronym,\n",
    "                             num_at,\n",
    "                             num_cap,\n",
    "                             num_digit,\n",
    "                             num_exclm,\n",
    "                             num_money,\n",
    "                             num_parenthesis,\n",
    "                             num_ques,\n",
    "                             num_tag,\n",
    "                             num_pic,\n",
    "                             is_start_num, \n",
    "                             is_superlative,\n",
    "#                              is_start5w1h\n",
    "                               ])\n",
    "title_feature = title_feature.T\n",
    "title_feature.rename(columns = {0:'num_acronym',\n",
    "                               1:'num_at',\n",
    "                               2:'num_cap',\n",
    "                               3:'num_digit',\n",
    "                               4:'num_exclm',\n",
    "                               5:'num_money',\n",
    "                               6:'num_parenthesis',\n",
    "                               7:'num_ques',\n",
    "                               8:'num_tag',\n",
    "                               9:'num_pic',\n",
    "                               10:'is_start_num',  \n",
    "                               11:'is_superlative',\n",
    "#                                10:'is_start5w1h'\n",
    "                              },inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_acronym</th>\n",
       "      <th>num_at</th>\n",
       "      <th>num_cap</th>\n",
       "      <th>num_digit</th>\n",
       "      <th>num_exclm</th>\n",
       "      <th>num_money</th>\n",
       "      <th>num_parenthesis</th>\n",
       "      <th>num_ques</th>\n",
       "      <th>num_tag</th>\n",
       "      <th>num_pic</th>\n",
       "      <th>is_start_num</th>\n",
       "      <th>is_superlative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>21997.000000</td>\n",
       "      <td>21997.000000</td>\n",
       "      <td>21997.000000</td>\n",
       "      <td>21997.000000</td>\n",
       "      <td>21997.000000</td>\n",
       "      <td>21997.000000</td>\n",
       "      <td>21997.000000</td>\n",
       "      <td>21997.000000</td>\n",
       "      <td>21997.000000</td>\n",
       "      <td>21997.000000</td>\n",
       "      <td>21997.000000</td>\n",
       "      <td>21997.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>7.717598</td>\n",
       "      <td>0.000409</td>\n",
       "      <td>0.019685</td>\n",
       "      <td>68.190526</td>\n",
       "      <td>0.022639</td>\n",
       "      <td>0.028868</td>\n",
       "      <td>0.028186</td>\n",
       "      <td>0.052553</td>\n",
       "      <td>0.002682</td>\n",
       "      <td>5.487612</td>\n",
       "      <td>-0.917989</td>\n",
       "      <td>-0.910170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>13.360093</td>\n",
       "      <td>0.020224</td>\n",
       "      <td>0.150540</td>\n",
       "      <td>128.193395</td>\n",
       "      <td>0.154158</td>\n",
       "      <td>0.179241</td>\n",
       "      <td>0.236523</td>\n",
       "      <td>0.258801</td>\n",
       "      <td>0.054294</td>\n",
       "      <td>10.244213</td>\n",
       "      <td>0.396615</td>\n",
       "      <td>0.414245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>380.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3388.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>370.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        num_acronym        num_at       num_cap     num_digit     num_exclm  \\\n",
       "count  21997.000000  21997.000000  21997.000000  21997.000000  21997.000000   \n",
       "mean       7.717598      0.000409      0.019685     68.190526      0.022639   \n",
       "std       13.360093      0.020224      0.150540    128.193395      0.154158   \n",
       "min        0.000000      0.000000      0.000000      4.000000      0.000000   \n",
       "25%        5.000000      0.000000      0.000000     47.000000      0.000000   \n",
       "50%        6.000000      0.000000      0.000000     57.000000      0.000000   \n",
       "75%        9.000000      0.000000      0.000000     68.000000      0.000000   \n",
       "max      380.000000      1.000000      3.000000   3388.000000      3.000000   \n",
       "\n",
       "          num_money  num_parenthesis      num_ques       num_tag  \\\n",
       "count  21997.000000     21997.000000  21997.000000  21997.000000   \n",
       "mean       0.028868         0.028186      0.052553      0.002682   \n",
       "std        0.179241         0.236523      0.258801      0.054294   \n",
       "min        0.000000         0.000000      0.000000      0.000000   \n",
       "25%        0.000000         0.000000      0.000000      0.000000   \n",
       "50%        0.000000         0.000000      0.000000      0.000000   \n",
       "75%        0.000000         0.000000      0.000000      0.000000   \n",
       "max        4.000000         4.000000     16.000000      2.000000   \n",
       "\n",
       "            num_pic  is_start_num  is_superlative  \n",
       "count  21997.000000  21997.000000    21997.000000  \n",
       "mean       5.487612     -0.917989       -0.910170  \n",
       "std       10.244213      0.396615        0.414245  \n",
       "min        0.000000     -1.000000       -1.000000  \n",
       "25%        1.000000     -1.000000       -1.000000  \n",
       "50%        2.000000     -1.000000       -1.000000  \n",
       "75%        6.000000     -1.000000       -1.000000  \n",
       "max      370.000000      1.000000        1.000000  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_feature.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Informality and Forward Reference features\n",
    "+ CLScore = 0.0588 * L - 0.296 * S - 15.8  \n",
    "\n",
    "   L = average number of letters     \n",
    "   S = average number of sentence per 100 words  \n",
    "   \n",
    "\n",
    "+ RIX = LW/S  \n",
    "\n",
    "+ LIX = W/S + (100*LW)/W  \n",
    "\n",
    "   W = number of words     \n",
    "   LW number of long words(7+ characters)     \n",
    "   S = number of sentence  \n",
    "   \n",
    "   \n",
    "+ Formality Measure (fmeasure):  \n",
    "\n",
    "   (nounfreq+adjectivefreq+prepositionfreq+particlefreq-pronounfreq-verbfreq-adverbfreq-interjectionfreq+100)*0.5   \n",
    "   \n",
    "\n",
    "+ Sentiment Analysis:  \n",
    "\n",
    "   extract polarity as it indicates the sentiment as value nearer to 1 means a positive sentiment and values nearer to -1 means a negative sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "RIX = []\n",
    "LIX = []\n",
    "sentiment = []\n",
    "CLScore = []\n",
    "f_measure = []\n",
    "\n",
    "for paragraph in data['targetParagraphs']:\n",
    "    \n",
    "    avg_letter = 0\n",
    "    avg_sentence = 0\n",
    "    num_word = 0\n",
    "    num_char = 0\n",
    "    num_sentence = 0\n",
    "    long_word = 0\n",
    "    noun_freq = 0\n",
    "    adjective_freq = 0\n",
    "    preposition_freq = 0\n",
    "    particle_freq = 0\n",
    "    pronoun_freq = 0\n",
    "    verb_freq = 0\n",
    "    adverb_freq = 0\n",
    "    interjection_freq = 0  \n",
    "    measure = 0\n",
    "    \n",
    "    paragraph = str(paragraph)\n",
    "    text = nltk.word_tokenize(paragraph)\n",
    "    part_of_speech = nltk.pos_tag(text)\n",
    "    for token,tag in part_of_speech:\n",
    "        if tag in ['NN','NNS','NNP','NNPS']:\n",
    "            noun_freq += 1\n",
    "        if tag in ['VB','VBD','VBG','VBN','VBP','VBZ']:\n",
    "            verb_freq += 1\n",
    "        if tag == 'UH':\n",
    "            interjection_freq += 1\n",
    "        if tag in ['RB','RBS','RBR']:\n",
    "            adverb_freq += 1\n",
    "        if tag == 'RP':\n",
    "            particle_freq += 1\n",
    "        if tag in ['JJ','JJR','JJS'] :\n",
    "            adjective_freq += 1\n",
    "        if tag == 'in':\n",
    "            preposition_freq += 1\n",
    "        if tag in ['WRB','WP$','WP','PRP$','PRP']:\n",
    "            pronoun_freq += 1\n",
    "    measure = (noun_freq+adjective_freq+preposition_freq+particle_freq-pronoun_freq-verb_freq-adverb_freq-interjection_freq)\n",
    "        \n",
    "    for word in paragraph.split():\n",
    "        num_word += 1\n",
    "        num_char += len(word)\n",
    "        if len(word) >= 7:\n",
    "            long_word += 1\n",
    "        for char in word:\n",
    "            if char == '.':\n",
    "                num_sentence += 1\n",
    "    \n",
    "    h_word = num_word//100\n",
    "    if h_word == 0:\n",
    "        h_word = 1\n",
    "    if num_sentence == 0:\n",
    "        num_sentence = 1\n",
    "    if num_word == 0:\n",
    "        num_word = 1\n",
    "\n",
    "    avg_letter = round((num_char / num_word), 1)\n",
    "    avg_sentence = round((num_sentence / h_word), 1)\n",
    "    RIX.append(round(long_word/num_sentence, 1))\n",
    "    CLScore.append(round(0.0588*avg_letter-0.296*avg_sentence-15.8, 1))\n",
    "    sentiment.append(round(TextBlob(paragraph).sentiment[0], 1))\n",
    "    LIX.append(round(num_word/num_sentence + (100*long_word)/num_word, 1))\n",
    "    f_measure.append((measure+100)/2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_feature = pd.DataFrame([RIX,\n",
    "                             LIX,\n",
    "                             sentiment,\n",
    "                             CLScore,\n",
    "                             f_measure,\n",
    "                               ])\n",
    "content_feature = content_feature.T\n",
    "content_feature.rename(columns = {0:'RIX',\n",
    "                               1:'LIX',\n",
    "                               2:'sentiment',\n",
    "                               3:'CLScore',\n",
    "                               4:'f_measure',\n",
    "                              },inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RIX</th>\n",
       "      <th>LIX</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>CLScore</th>\n",
       "      <th>f_measure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>21997.000000</td>\n",
       "      <td>21997.000000</td>\n",
       "      <td>21997.000000</td>\n",
       "      <td>21997.000000</td>\n",
       "      <td>21997.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.565254</td>\n",
       "      <td>47.946152</td>\n",
       "      <td>0.095872</td>\n",
       "      <td>-17.396790</td>\n",
       "      <td>96.145906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.774795</td>\n",
       "      <td>12.432667</td>\n",
       "      <td>0.110498</td>\n",
       "      <td>0.930461</td>\n",
       "      <td>69.958521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-35.200000</td>\n",
       "      <td>-521.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>42.300000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-17.700000</td>\n",
       "      <td>63.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.200000</td>\n",
       "      <td>47.500000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>-17.300000</td>\n",
       "      <td>81.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.500000</td>\n",
       "      <td>52.500000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>-16.900000</td>\n",
       "      <td>111.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>142.000000</td>\n",
       "      <td>515.200000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-10.100000</td>\n",
       "      <td>3373.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                RIX           LIX     sentiment       CLScore     f_measure\n",
       "count  21997.000000  21997.000000  21997.000000  21997.000000  21997.000000\n",
       "mean       5.565254     47.946152      0.095872    -17.396790     96.145906\n",
       "std        3.774795     12.432667      0.110498      0.930461     69.958521\n",
       "min        0.000000      1.000000     -1.000000    -35.200000   -521.500000\n",
       "25%        4.000000     42.300000      0.000000    -17.700000     63.000000\n",
       "50%        5.200000     47.500000      0.100000    -17.300000     81.500000\n",
       "75%        6.500000     52.500000      0.100000    -16.900000    111.000000\n",
       "max      142.000000    515.200000      1.000000    -10.100000   3373.500000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_feature.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Similarity between Title and Top 5 Sentences\n",
    "\n",
    "Similarity between the title and the top one,two,three,four and five sentences of the body. Using tf-idf encoding to compute the similarity and removed stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "top1sim = []\n",
    "top2sim = []\n",
    "top3sim = []\n",
    "top4sim = []\n",
    "top5sim = []\n",
    "\n",
    "for i in range(len(data)):\n",
    "    \n",
    "    title = data['targetTitle'][i]\n",
    "    body = data['targetParagraphs'][i]\n",
    "    sim_1 = sim_2 = sim_3 = sim_4 = sim_5 = 0.0\n",
    "    sen_1 = sen_2 = sen_3 = sen_4 = sen_5 = ''\n",
    "    \n",
    "    if len(body) == 0:\n",
    "        top1sim.append(sim_1)\n",
    "        top2sim.append(sim_2)\n",
    "        top3sim.append(sim_3)\n",
    "        top4sim.append(sim_4)\n",
    "        top5sim.append(sim_5)\n",
    "        continue\n",
    "    if len(body) >= 1:\n",
    "        sen_1 = body[0]\n",
    "    if len(body) >= 2:\n",
    "        sen_2 = body[1]\n",
    "    if len(body) >= 3:\n",
    "        sen_3 = body[2]\n",
    "    if len(body) >= 4:\n",
    "        sen_4 = body[3]\n",
    "    if len(body) >= 5:\n",
    "        sen_5 = body[4]\n",
    "    \n",
    "    essay = str(title + ' ' + sen_1 + ' ' + sen_2 + ' '  + sen_3 + ' ' + sen_4 + ' ' + sen_5)\n",
    "    tfidf = TfidfVectorizer(lowercase = True, analyzer = 'word', stop_words = 'english',ngram_range = (1,1))\n",
    "    train_vect = tfidf.fit_transform([essay])    \n",
    "    title_vect = tfidf.transform([title])\n",
    "    \n",
    "    s1_vect = tfidf.transform([sen_1])\n",
    "    sim_1 = round(float(cosine_similarity(title_vect,s1_vect)),1)\n",
    "    top1sim.append(sim_1)\n",
    "    \n",
    "    s2_vect = tfidf.transform([sen_2])\n",
    "    sim_2 = round(float(cosine_similarity(title_vect,s2_vect)),1)\n",
    "    top2sim.append(sim_2)\n",
    "    \n",
    "    s3_vect = tfidf.transform([sen_3])\n",
    "    sim_3 = round(float(cosine_similarity(title_vect,s3_vect)),1)\n",
    "    top3sim.append(sim_3)\n",
    "    \n",
    "    s4_vect = tfidf.transform([sen_4])\n",
    "    sim_4 = round(float(cosine_similarity(title_vect,s4_vect)),1)\n",
    "    top4sim.append(sim_4)\n",
    "    \n",
    "    s5_vect = tfidf.transform([sen_5])\n",
    "    sim_5 = round(float(cosine_similarity(title_vect,s5_vect)),1)\n",
    "    top5sim.append(sim_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_feature = pd.DataFrame([top1sim,\n",
    "                             top2sim,\n",
    "                             top3sim,\n",
    "                             top4sim,\n",
    "                             top5sim,\n",
    "                               ])\n",
    "sim_feature = sim_feature.T\n",
    "sim_feature.rename(columns = {0:'top1sim',\n",
    "                               1:'top2sim',\n",
    "                               2:'top3sim',\n",
    "                               3:'top4sim',\n",
    "                               4:'top5sim',\n",
    "                              },inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>top1sim</th>\n",
       "      <th>top2sim</th>\n",
       "      <th>top3sim</th>\n",
       "      <th>top4sim</th>\n",
       "      <th>top5sim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>21997.000000</td>\n",
       "      <td>21997.000000</td>\n",
       "      <td>21997.000000</td>\n",
       "      <td>21997.00000</td>\n",
       "      <td>21997.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.237628</td>\n",
       "      <td>0.166332</td>\n",
       "      <td>0.126017</td>\n",
       "      <td>0.11778</td>\n",
       "      <td>0.100714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.208294</td>\n",
       "      <td>0.166482</td>\n",
       "      <td>0.139019</td>\n",
       "      <td>0.13668</td>\n",
       "      <td>0.122919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.10000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.20000</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            top1sim       top2sim       top3sim      top4sim       top5sim\n",
       "count  21997.000000  21997.000000  21997.000000  21997.00000  21997.000000\n",
       "mean       0.237628      0.166332      0.126017      0.11778      0.100714\n",
       "std        0.208294      0.166482      0.139019      0.13668      0.122919\n",
       "min        0.000000      0.000000      0.000000      0.00000      0.000000\n",
       "25%        0.000000      0.000000      0.000000      0.00000      0.000000\n",
       "50%        0.200000      0.100000      0.100000      0.10000      0.100000\n",
       "75%        0.400000      0.300000      0.200000      0.20000      0.200000\n",
       "max        1.000000      1.000000      1.000000      1.00000      1.000000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_feature.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Basic Pre-processing\n",
    "\n",
    "## 3.1 Lower case Removing Punctuation Removal of Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_corpus = stopwords.words('english')\n",
    "stop_words = []\n",
    "caption = []\n",
    "content = []\n",
    "title = []\n",
    "\n",
    "for sentence in data['targetParagraphs']:\n",
    "    sentence = str(sentence).replace('[^\\w\\s]','')\n",
    "    sentence = re.sub('[\\u0060|\\u0021-\\u002c|\\u002e-\\u002f|\\u003a-\\u003f|\\u2200-\\u22ff|\\uFB00-\\uFFFD|\\u2E80-\\u33FF]',' ',sentence)\n",
    "    stop_cnt = 0\n",
    "    string = ''\n",
    "#     sentence = TextBlob(sentence).correct()\n",
    "    for word in sentence.split():\n",
    "        if word in en_corpus:\n",
    "            stop_cnt += 1\n",
    "        if word not in en_corpus:            \n",
    "            string += ' ' + word.lower()\n",
    "    stop_words.append(stop_cnt)\n",
    "    content.append(string)    \n",
    "\n",
    "for sentence in data['targetCaptions']:\n",
    "    string = ''\n",
    "    sentence = str(sentence).replace('[^\\w\\s]','')\n",
    "    sentence = re.sub('[\\u0060|\\u0021-\\u002c|\\u002e-\\u002f|\\u003a-\\u003f|\\u2200-\\u22ff|\\uFB00-\\uFFFD|\\u2E80-\\u33FF]',' ',sentence)\n",
    "    for word in sentence.split():\n",
    "        if word not in en_corpus:            \n",
    "            string += ' ' + word.lower()\n",
    "    caption.append(string) \n",
    "\n",
    "for sentence in data['targetTitle']:\n",
    "    string = ''\n",
    "    sentence = str(sentence).replace('[^\\w\\s]','')\n",
    "    sentence = re.sub('[\\u0060|\\u0021-\\u002c|\\u002e-\\u002f|\\u003a-\\u003f|\\u2200-\\u22ff|\\uFB00-\\uFFFD|\\u2E80-\\u33FF]',' ',sentence)\n",
    "    for word in sentence.split():\n",
    "        if word not in en_corpus:            \n",
    "            string += ' ' + word.lower()\n",
    "    title.append(string)\n",
    "    \n",
    "corpus = title + content + caption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Common / Rare words removal & Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_caption = []\n",
    "target_content = []\n",
    "target_title = []\n",
    "\n",
    "common = pd.Series(' '.join(caption).split()).value_counts()[:10]\n",
    "rare = pd.Series(' '.join(caption).split()).value_counts()[-10:]  \n",
    "\n",
    "for sentence in caption:\n",
    "    string = ''\n",
    "    lemma = ''\n",
    "    sentence = str(sentence)\n",
    "    for word in sentence.split():\n",
    "        if word not in common:\n",
    "            if word  not in rare:\n",
    "                lemma = Word(word).lemmatize()\n",
    "                string += ' ' + lemma\n",
    "    target_caption.append(string)\n",
    "\n",
    "common = pd.Series(' '.join(content).split()).value_counts()[:10]\n",
    "rare = pd.Series(' '.join(content).split()).value_counts()[-10:]  \n",
    "\n",
    "for sentence in content:\n",
    "    string = ''\n",
    "    sentence = str(sentence)\n",
    "    for word in sentence.split():\n",
    "        if word not in common:\n",
    "            if word  not in rare:\n",
    "                lemma = Word(word).lemmatize()\n",
    "                string += ' ' + lemma\n",
    "    target_content.append(string)\n",
    "    \n",
    "common = pd.Series(' '.join(title).split()).value_counts()[:10]\n",
    "rare = pd.Series(' '.join(title).split()).value_counts()[-10:]  \n",
    "\n",
    "for sentence in title:\n",
    "    string = ''\n",
    "    sentence = str(sentence)\n",
    "    for word in sentence.split():\n",
    "        if word not in common:\n",
    "            if word  not in rare:\n",
    "                lemma = Word(word).lemmatize()\n",
    "                string += ' ' + lemma\n",
    "    target_title.append(string) \\\n",
    "    \n",
    "target_corpus = []\n",
    "for i in range(len(target_content)):\n",
    "    total_context = str(target_caption[i] + target_content[i] + target_title[i])\n",
    "    target_corpus.append(total_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.Advance Text Processing\n",
    "## 4.1 N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WordList(['tony', 'nominees']),\n",
       " WordList(['nominees', 'craziest']),\n",
       " WordList(['craziest', 'moments']),\n",
       " WordList(['moments', 'stage'])]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextBlob(corpus[0]).ngrams(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(max_features = 1024, lowercase = True, analyzer = 'word', stop_words = 'english',ngram_range = (1,1))\n",
    "corpus_vect = tfidf.fit_transform(target_corpus)\n",
    "\n",
    "vec_feature = []\n",
    "\n",
    "for article in target_corpus:\n",
    "    corpus_vect = tfidf.transform([article])\n",
    "    vec_feature.append(corpus_vect)\n",
    "\n",
    "tfidf_feature = []\n",
    "\n",
    "for index in vec_feature:\n",
    "    tmp = index.toarray().reshape(1024)\n",
    "    tfidf_feature.append(tmp)\n",
    "    \n",
    "tfidf_feature = pd.DataFrame(tfidf_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<21997x1000 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 2535929 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow = CountVectorizer(max_features=1000, lowercase=True, ngram_range=(1,1),analyzer = \"word\")\n",
    "train_bow = bow.fit_transform(target_corpus)\n",
    "\n",
    "train_bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glove_input_file = 'glove.840B.300d.txt'\n",
    "# word2vec_output_file = 'glove.840B.300d.txt.word2vec'\n",
    "# glove2word2vec(glove_input_file, word2vec_output_file)\n",
    "# filename = 'glove.840B.300d.txt.word2vec'\n",
    "# model = KeyedVectors.load_word2vec_format(filename, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = 'glove.840B.300d.txt.word2vec'\n",
    "# model = KeyedVectors.load_word2vec_format(filename, binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(3),\n",
    "    SVC(probability=True),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    AdaBoostClassifier(),\n",
    "    GradientBoostingClassifier(),\n",
    "    GaussianNB(),\n",
    "    LinearDiscriminantAnalysis(),\n",
    "    QuadraticDiscriminantAnalysis(),\n",
    "    LogisticRegression()]\n",
    "\n",
    "classifiers_name = [\n",
    "    \"KNeighborsClassifier\",\n",
    "    \"SVC\",\n",
    "   \" DecisionTreeClassifier\",\n",
    "    \"RandomForestClassifier\",\n",
    "    \"AdaBoostClassifier\",\n",
    "    \"GradientBoostingClassifier\",\n",
    "    \"GaussianNB\",\n",
    "    \"LinearDiscriminantAnalysis\",\n",
    "    \"QuadraticDiscriminantAnalysis\",\n",
    "    \"LogisticRegression\"]\n",
    "\n",
    "log_cols = ['Classifier','Accuracy']\n",
    "log = pd.DataFrame(columns = log_cols)\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits = 10, test_size = 0.1, random_state = 0)\n",
    "\n",
    "# acc_dict = {}\n",
    "# mse_dict = {}\n",
    "\n",
    "# handy_feature = pd.concat([char_feature,title_feature,content_feature,sim_feature,tfidf_feature], axis = 1)\n",
    "# y = []\n",
    "# for label in data['truthMean']:\n",
    "#     y.append(math.floor(label*10))\n",
    "# X = np.array(handy_feature)\n",
    "# y = np.array(y)\n",
    "\n",
    "# for train_index, test_index in sss.split(X,y):\n",
    "    \n",
    "#     print(\"Train Index:\\n\",train_index,\"\\nTest Index:\\n\",test_index)  \n",
    "    \n",
    "#     X_train, X_test = X[train_index],X[test_index]\n",
    "#     y_train, y_test = y[train_index],y[test_index]\n",
    "    \n",
    "#     for clf in classifiers:\n",
    "#         name = clf.__class__.__name__\n",
    "#         clf.fit(X_train,y_train)\n",
    "#         train_predictions = clf.predict(X_test)\n",
    "#         acc = accuracy_score(y_test,train_predictions)\n",
    "#         mse = mean_squared_error(y_test, y_pred)\n",
    "#         if name in acc_dict:\n",
    "#             acc_dict[name] += acc\n",
    "#         else:\n",
    "#             acc_dict[name] = acc\n",
    "        \n",
    "#         if name in mse_dict:\n",
    "#             mse_dict[name] += mse\n",
    "#         else:\n",
    "#             mse_dict[name] = mse\n",
    "# for clf in acc_dict:\n",
    "#     acc_dict[clf] = acc_dict\n",
    "#     log_entry = pd.DataFrame([[clf,acc_dict[clf]]],columns = log_cols)\n",
    "    \n",
    "# from sklearn.metrics import accuracy_score, log_loss\n",
    "# from sklearn.metricsics import mean_squared_errorprint(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Index:\n",
      " [17656 16400  4252 ... 16772 17094 11542] \n",
      "Test Index:\n",
      " [10864 20899 17734 ...  7067  4378  5115]\n",
      "0.7840909090909091 0.8636363636363636 0.6282130879458966\n",
      "[[1551  104]\n",
      " [ 371  174]]\n",
      "Train Index:\n",
      " [11013  3244 12404 ... 11979   978  9197] \n",
      "Test Index:\n",
      " [ 1268 17329 19377 ...  6354  1835  9929]\n",
      "0.8 0.8 0.6560159649657695\n",
      "[[1558   97]\n",
      " [ 343  202]]\n",
      "Train Index:\n",
      " [  750 13429  6061 ...  1253 20203 18677] \n",
      "Test Index:\n",
      " [ 4030 18640 19072 ... 11712 14646 19399]\n",
      "0.8086363636363636 0.7654545454545455 0.6777543723495663\n",
      "[[1551  104]\n",
      " [ 317  228]]\n",
      "Train Index:\n",
      " [10165 20025  4695 ... 18858  8116  4754] \n",
      "Test Index:\n",
      " [ 2439 10864 14900 ... 21995 20204  3093]\n",
      "0.8127272727272727 0.7490909090909091 0.678627456415089\n",
      "[[1563   92]\n",
      " [ 320  225]]\n",
      "Train Index:\n",
      " [10971 11322  6202 ...  8048 19030 21690] \n",
      "Test Index:\n",
      " [18079  7051 11968 ...  6594 17319 16912]\n",
      "0.8072727272727273 0.7709090909090909 0.6780786607167605\n",
      "[[1546  109]\n",
      " [ 315  230]]\n",
      "Train Index:\n",
      " [ 6575  6777 17445 ... 20686 14031  2523] \n",
      "Test Index:\n",
      " [ 5640 12625 20131 ... 18748  8041 10887]\n",
      "0.8113636363636364 0.7545454545454545 0.6746445300590371\n",
      "[[1565   90]\n",
      " [ 325  220]]\n",
      "Train Index:\n",
      " [ 9161 14766 20648 ...  3093 10135 13288] \n",
      "Test Index:\n",
      " [ 4089  8322 14688 ...  1427 19610 15993]\n",
      "0.8040909090909091 0.7836363636363637 0.6667341112558552\n",
      "[[1554  101]\n",
      " [ 330  215]]\n",
      "Train Index:\n",
      " [10726 17321 18548 ...  1498 19947 18052] \n",
      "Test Index:\n",
      " [16460  9395 14750 ...  9568 12212 16211]\n",
      "0.81 0.76 0.6749688184262315\n",
      "[[1560   95]\n",
      " [ 323  222]]\n",
      "Train Index:\n",
      " [ 8934  6450 11921 ... 16473 17219 20525] \n",
      "Test Index:\n",
      " [16812 10260 13609 ...  2818  9372  3248]\n",
      "0.8063636363636364 0.7745454545454545 0.6670140524959117\n",
      "[[1561   94]\n",
      " [ 332  213]]\n",
      "Train Index:\n",
      " [ 2073 16594 10101 ... 13272 12164 13756] \n",
      "Test Index:\n",
      " [13432 17026 21324 ...  5917 11766  2021]\n",
      "0.8040909090909091 0.7836363636363637 0.6648881620887497\n",
      "[[1557   98]\n",
      " [ 333  212]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score,log_loss,mean_squared_error,precision_score,f1_score,confusion_matrix\n",
    "from sklearn import metrics\n",
    "\n",
    "handy_feature = pd.concat([title_feature,content_feature,sim_feature,tfidf_feature], axis = 1)\n",
    "y = []\n",
    "for label in data['truthMean']:\n",
    "    if label >= 0.5:\n",
    "        y.append(1)\n",
    "    else:\n",
    "        y.append(-1)\n",
    "X = np.array(handy_feature)\n",
    "y = np.array(y)\n",
    "\n",
    "\n",
    "for train_index, test_index in sss.split(X,y):\n",
    "    \n",
    "    print(\"Train Index:\\n\",train_index,\"\\nTest Index:\\n\",test_index)  \n",
    "    \n",
    "    X_train, X_test = X[train_index],X[test_index]\n",
    "    y_train, y_test = y[train_index],y[test_index]\n",
    "    \n",
    "    candidate_classifier.fit(X_train,y_train)\n",
    "    train_predictions = candidate_classifier.predict(X_test)\n",
    "    acc = accuracy_score(y_test,train_predictions)\n",
    "    mse = mean_squared_error(y_test, train_predictions)\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_test, train_predictions)\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "    print(acc,mse,auc)\n",
    "    print(confusion_matrix(y_test, train_predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
