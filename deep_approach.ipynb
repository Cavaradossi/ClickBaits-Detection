{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "from textblob import Word\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import math\n",
    "import nltk\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "def read_truth(path):\n",
    "    data = []\n",
    "    index = []\n",
    "    for i, line in enumerate(open(path + '/truth.jsonl', 'r')):\n",
    "        instance = json.loads(line)\n",
    "        data.append(instance)\n",
    "        index.append(instance['id'])\n",
    "    df = pd.DataFrame(data=data, index=index)\n",
    "    df.sort_index(inplace=True)\n",
    "    return df\n",
    "\n",
    "def read_instance(path):\n",
    "    data = []\n",
    "    index = []\n",
    "    for i, line in enumerate(open(path + '/instances.jsonl', 'rb')):\n",
    "        instance = json.loads(line)\n",
    "        data.append(instance)\n",
    "        index.append(instance['id'])\n",
    "    df = pd.DataFrame(data=data, index=index)\n",
    "    df.sort_index(inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "train_truth = read_truth('./clickbait17-train-170331')\n",
    "train_instances = read_instance('./clickbait17-train-170331')\n",
    "validation_truth = read_truth('./clickbait17-validation-170630')\n",
    "validation_instances = read_instance('./clickbait17-validation-170630')\n",
    "train = pd.merge(train_truth, train_instances, on = 'id')\n",
    "validation = pd.merge(validation_truth, validation_instances, on = 'id')\n",
    "data = pd.concat([train, validation],ignore_index = True)\n",
    "data['truthClass'] = data['truthClass'].map({'clickbait':True ,'no-clickbait':False}).astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import warnings\n",
    "warnings.simplefilter( \"ignore\", UserWarning)\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "import numpy as np\n",
    "from string import punctuation\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score , auc , roc_auc_score\n",
    "from sklearn.externals import joblib\n",
    "import scipy\n",
    "from scipy.sparse import hstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_corpus = stopwords.words('english')\n",
    "stop_words = []\n",
    "caption = []\n",
    "content = []\n",
    "title = []\n",
    "\n",
    "for sentence in data['targetParagraphs']:\n",
    "    sentence = str(sentence).replace('[^\\w\\s]','')\n",
    "    sentence = re.sub('[\\u0060|\\u0021-\\u002c|\\u002e-\\u002f|\\u003a-\\u003f|\\u2200-\\u22ff|\\uFB00-\\uFFFD|\\u2E80-\\u33FF]',' ',sentence)\n",
    "    sentence = re.sub( r'httpS+', '', sentence)\n",
    "    sentence = re.sub( r\"#(w+)\", '', sentence)\n",
    "    sentence = re.sub( r\"@(w+)\", '', sentence)\n",
    "    stop_cnt = 0\n",
    "    string = ''\n",
    "#     sentence = TextBlob(sentence).correct()\n",
    "    for word in sentence.split():\n",
    "        if word in en_corpus:\n",
    "            stop_cnt += 1\n",
    "        if word not in en_corpus:            \n",
    "            string += ' ' + word.lower()\n",
    "    stop_words.append(stop_cnt)\n",
    "    content.append(string)    \n",
    "\n",
    "for sentence in data['targetCaptions']:\n",
    "    string = ''\n",
    "    sentence = str(sentence).replace('[^\\w\\s]','')\n",
    "    sentence = re.sub('[\\u0060|\\u0021-\\u002c|\\u002e-\\u002f|\\u003a-\\u003f|\\u2200-\\u22ff|\\uFB00-\\uFFFD|\\u2E80-\\u33FF]',' ',sentence)\n",
    "    sentence = re.sub( r'httpS+', '', sentence)\n",
    "    sentence = re.sub( r\"#(w+)\", '', sentence)\n",
    "    sentence = re.sub( r\"@(w+)\", '', sentence)\n",
    "    for word in sentence.split():\n",
    "        if word not in en_corpus:            \n",
    "            string += ' ' + word.lower()\n",
    "    caption.append(string) \n",
    "\n",
    "for sentence in data['targetTitle']:\n",
    "    string = ''\n",
    "    sentence = str(sentence).replace('[^\\w\\s]','')\n",
    "    sentence = re.sub('[\\u0060|\\u0021-\\u002c|\\u002e-\\u002f|\\u003a-\\u003f|\\u2200-\\u22ff|\\uFB00-\\uFFFD|\\u2E80-\\u33FF]',' ',sentence)\n",
    "    sentence = re.sub( r'httpS+', '', sentence)\n",
    "    sentence = re.sub( r\"#(w+)\", '', sentence)\n",
    "    sentence = re.sub( r\"@(w+)\", '', sentence)\n",
    "    for word in sentence.split():\n",
    "        if word not in en_corpus:            \n",
    "            string += ' ' + word.lower()\n",
    "    title.append(string)\n",
    "    \n",
    "corpus = title + content + caption\n",
    "\n",
    "\n",
    "target_caption = []\n",
    "target_content = []\n",
    "target_title = []\n",
    "\n",
    "common = pd.Series(' '.join(caption).split()).value_counts()[:10]\n",
    "rare = pd.Series(' '.join(caption).split()).value_counts()[-10:]  \n",
    "\n",
    "for sentence in caption:\n",
    "    string = ''\n",
    "    lemma = ''\n",
    "    sentence = str(sentence)\n",
    "    for word in sentence.split():\n",
    "        if word not in common:\n",
    "            if word  not in rare:\n",
    "                lemma = Word(word).lemmatize()\n",
    "                string += ' ' + lemma\n",
    "    target_caption.append(string)\n",
    "\n",
    "common = pd.Series(' '.join(content).split()).value_counts()[:10]\n",
    "rare = pd.Series(' '.join(content).split()).value_counts()[-10:]  \n",
    "\n",
    "for sentence in content:\n",
    "    string = ''\n",
    "    sentence = str(sentence)\n",
    "    for word in sentence.split():\n",
    "        if word not in common:\n",
    "            if word  not in rare:\n",
    "                lemma = Word(word).lemmatize()\n",
    "                string += ' ' + lemma\n",
    "    target_content.append(string)\n",
    "    \n",
    "common = pd.Series(' '.join(title).split()).value_counts()[:10]\n",
    "rare = pd.Series(' '.join(title).split()).value_counts()[-10:]  \n",
    "\n",
    "for sentence in title:\n",
    "    string = ''\n",
    "    sentence = str(sentence)\n",
    "    for word in sentence.split():\n",
    "        if word not in common:\n",
    "            if word  not in rare:\n",
    "                lemma = Word(word).lemmatize()\n",
    "                string += ' ' + lemma\n",
    "    target_title.append(string) \n",
    "    \n",
    "target_corpus = []\n",
    "for i in range(len(target_content)):\n",
    "    total_context = target_caption[i] + target_content[i] + target_title[i]\n",
    "    total_context = str(total_context).replace('[','').replace(']','').replace('\\\\','')\n",
    "    target_corpus.append(total_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_input_file = 'glove.840B.300d.txt'\n",
    "word2vec_output_file = 'glove.840B.300d.txt.word2vec'\n",
    "glove2word2vec(glove_input_file, word2vec_output_file)\n",
    "filename = 'glove.840B.300d.txt.word2vec'\n",
    "w2v_model = KeyedVectors.load_word2vec_format(filename, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten, Dropout\n",
    "from keras.layers import LSTM, Embedding\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 100\n",
    "EMBEDDING_DIM = 300\n",
    "VALIDATION_SPLIT = 0.1\n",
    "TEST_SPLIT = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of label tensor: (21997, 2)\n"
     ]
    }
   ],
   "source": [
    "label = []\n",
    "for score in data['truthMean']:\n",
    "    if score >= 0.5:\n",
    "        label.append(1)\n",
    "    else:\n",
    "        label.append(-1)\n",
    "label = np.array(label)\n",
    "label = to_categorical(np.asarray(label))\n",
    "print('Shape of label tensor:', label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 215910 unique tokens.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'all_labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-d3475c0604b0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Found %s unique tokens.'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mMAX_SEQUENCE_LENGTH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Shape of data tensor:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Shape of label tensor:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'all_labels' is not defined"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(target_corpus)\n",
    "sequences = tokenizer.texts_to_sequences(target_corpus)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels = to_categorical(np.asarray(all_labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "not_in_model = 0\n",
    "in_model = 0\n",
    "for word, i in word_index.items():\n",
    "    if word in w2v_model:\n",
    "        in_model += 1\n",
    "        embedding_matrix[i] = np.asarray(w2v_model[word], dtype='float32')\n",
    "    else:\n",
    "        not_in_model += 1\n",
    "print (str(not_in_model)+' words not in w2v model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = int(len(data)*(1-VALIDATION_SPLIT-TEST_SPLIT))\n",
    "p2 = int(len(data)*(1-TEST_SPLIT))\n",
    "x_train = data[:p1]\n",
    "y_train = label[:p1]\n",
    "x_val = data[p1:p2]\n",
    "y_val = label[p1:p2]\n",
    "x_test = data[p2:]\n",
    "y_test = label[p2:]\n",
    "print ('train docs: '+str(len(x_train)))\n",
    "print ('val docs: '+str(len(x_val)))\n",
    "print ('test docs: '+str(len(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(LSTM(200, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(label.shape[1], activation='softmax'))\n",
    "model.summary()\n",
    "#plot_model(model, to_file='model.png',show_shapes=True)\n",
    " \n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "print (model.metrics_names)\n",
    "model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=2, batch_size=128)\n",
    "print (model.evaluate(x_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
