{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Loading JSON dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "from textblob import Word\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import math\n",
    "import nltk\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "def read_truth(path):\n",
    "    data = []\n",
    "    index = []\n",
    "    for i, line in enumerate(open(path + '/truth.jsonl', 'r')):\n",
    "        instance = json.loads(line)\n",
    "        data.append(instance)\n",
    "        index.append(instance['id'])\n",
    "    df = pd.DataFrame(data=data, index=index)\n",
    "    df.sort_index(inplace=True)\n",
    "    return df\n",
    "\n",
    "def read_instance(path):\n",
    "    data = []\n",
    "    index = []\n",
    "    for i, line in enumerate(open(path + '/instances.jsonl', 'rb')):\n",
    "        instance = json.loads(line)\n",
    "        data.append(instance)\n",
    "        index.append(instance['id'])\n",
    "    df = pd.DataFrame(data=data, index=index)\n",
    "    df.sort_index(inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "train_truth = read_truth('./clickbait17-train-170331')\n",
    "train_instances = read_instance('./clickbait17-train-170331')\n",
    "validation_truth = read_truth('./clickbait17-validation-170630')\n",
    "validation_instances = read_instance('./clickbait17-validation-170630')\n",
    "train = pd.merge(train_truth, train_instances, on = 'id')\n",
    "validation = pd.merge(validation_truth, validation_instances, on = 'id')\n",
    "data = pd.concat([train, validation],ignore_index = True)\n",
    "data['truthClass'] = data['truthClass'].map({'clickbait':True ,'no-clickbait':False}).astype(bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import warnings\n",
    "warnings.simplefilter( \"ignore\", UserWarning)\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "import numpy as np\n",
    "from string import punctuation\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score , auc , roc_auc_score\n",
    "from sklearn.externals import joblib\n",
    "import scipy\n",
    "from scipy.sparse import hstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_corpus = stopwords.words('english')\n",
    "stop_words = []\n",
    "caption = []\n",
    "content = []\n",
    "title = []\n",
    "\n",
    "for sentence in data['targetParagraphs']:\n",
    "    sentence = str(sentence).replace('[^\\w\\s]','')\n",
    "    sentence = re.sub('[\\u0060|\\u0021-\\u002c|\\u002e-\\u002f|\\u003a-\\u003f|\\u2200-\\u22ff|\\uFB00-\\uFFFD|\\u2E80-\\u33FF]',' ',sentence)\n",
    "    sentence = re.sub( r'httpS+', '', sentence)\n",
    "    sentence = re.sub( r\"#(w+)\", '', sentence)\n",
    "    sentence = re.sub( r\"@(w+)\", '', sentence)\n",
    "    stop_cnt = 0\n",
    "    string = ''\n",
    "#     sentence = TextBlob(sentence).correct()\n",
    "    for word in sentence.split():\n",
    "        if word in en_corpus:\n",
    "            stop_cnt += 1\n",
    "        if word not in en_corpus:            \n",
    "            string += ' ' + word.lower()\n",
    "    stop_words.append(stop_cnt)\n",
    "    content.append(string)    \n",
    "\n",
    "for sentence in data['targetCaptions']:\n",
    "    string = ''\n",
    "    sentence = str(sentence).replace('[^\\w\\s]','')\n",
    "    sentence = re.sub('[\\u0060|\\u0021-\\u002c|\\u002e-\\u002f|\\u003a-\\u003f|\\u2200-\\u22ff|\\uFB00-\\uFFFD|\\u2E80-\\u33FF]',' ',sentence)\n",
    "    sentence = re.sub( r'httpS+', '', sentence)\n",
    "    sentence = re.sub( r\"#(w+)\", '', sentence)\n",
    "    sentence = re.sub( r\"@(w+)\", '', sentence)\n",
    "    for word in sentence.split():\n",
    "        if word not in en_corpus:            \n",
    "            string += ' ' + word.lower()\n",
    "    caption.append(string) \n",
    "\n",
    "for sentence in data['targetTitle']:\n",
    "    string = ''\n",
    "    sentence = str(sentence).replace('[^\\w\\s]','')\n",
    "    sentence = re.sub('[\\u0060|\\u0021-\\u002c|\\u002e-\\u002f|\\u003a-\\u003f|\\u2200-\\u22ff|\\uFB00-\\uFFFD|\\u2E80-\\u33FF]',' ',sentence)\n",
    "    sentence = re.sub( r'httpS+', '', sentence)\n",
    "    sentence = re.sub( r\"#(w+)\", '', sentence)\n",
    "    sentence = re.sub( r\"@(w+)\", '', sentence)\n",
    "    for word in sentence.split():\n",
    "        if word not in en_corpus:            \n",
    "            string += ' ' + word.lower()\n",
    "    title.append(string)\n",
    "    \n",
    "corpus = title + content + caption\n",
    "\n",
    "\n",
    "target_caption = []\n",
    "target_content = []\n",
    "target_title = []\n",
    "\n",
    "common = pd.Series(' '.join(caption).split()).value_counts()[:10]\n",
    "rare = pd.Series(' '.join(caption).split()).value_counts()[-10:]  \n",
    "\n",
    "for sentence in caption:\n",
    "    string = ''\n",
    "    lemma = ''\n",
    "    sentence = str(sentence)\n",
    "    for word in sentence.split():\n",
    "        if word not in common:\n",
    "            if word  not in rare:\n",
    "                lemma = Word(word).lemmatize()\n",
    "                string += ' ' + lemma\n",
    "    target_caption.append(string)\n",
    "\n",
    "common = pd.Series(' '.join(content).split()).value_counts()[:10]\n",
    "rare = pd.Series(' '.join(content).split()).value_counts()[-10:]  \n",
    "\n",
    "for sentence in content:\n",
    "    string = ''\n",
    "    sentence = str(sentence)\n",
    "    for word in sentence.split():\n",
    "        if word not in common:\n",
    "            if word  not in rare:\n",
    "                lemma = Word(word).lemmatize()\n",
    "                string += ' ' + lemma\n",
    "    target_content.append(string)\n",
    "    \n",
    "common = pd.Series(' '.join(title).split()).value_counts()[:10]\n",
    "rare = pd.Series(' '.join(title).split()).value_counts()[-10:]  \n",
    "\n",
    "for sentence in title:\n",
    "    string = ''\n",
    "    sentence = str(sentence)\n",
    "    for word in sentence.split():\n",
    "        if word not in common:\n",
    "            if word  not in rare:\n",
    "                lemma = Word(word).lemmatize()\n",
    "                string += ' ' + lemma\n",
    "    target_title.append(string) \n",
    "    \n",
    "target_corpus = []\n",
    "for i in range(len(target_content)):\n",
    "    total_context = target_caption[i] + target_content[i] + target_title[i]\n",
    "    total_context = str(total_context).replace('[','').replace(']','').replace('\\\\','')\n",
    "    target_corpus.append(total_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_input_file = 'glove.840B.300d.txt'\n",
    "word2vec_output_file = 'glove.840B.300d.txt.word2vec'\n",
    "glove2word2vec(glove_input_file, word2vec_output_file)\n",
    "filename = 'glove.840B.300d.txt.word2vec'\n",
    "w2v_model = KeyedVectors.load_word2vec_format(filename, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten, Dropout\n",
    "from keras.layers import LSTM, Embedding\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 100\n",
    "EMBEDDING_DIM = 300\n",
    "VALIDATION_SPLIT = 0.1\n",
    "TEST_SPLIT = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of label tensor: (21997, 2)\n"
     ]
    }
   ],
   "source": [
    "label = []\n",
    "for score in data['truthMean']:\n",
    "    if score >= 0.5:\n",
    "        label.append(1)\n",
    "    else:\n",
    "        label.append(-1)\n",
    "label = np.array(label)\n",
    "label = to_categorical(np.asarray(label))\n",
    "print('Shape of label tensor:', label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 215909 unique tokens.\n",
      "Shape of data tensor: (21997, 100)\n",
      "Shape of label tensor: (21997, 2)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(target_corpus)\n",
    "sequences = tokenizer.texts_to_sequences(target_corpus)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128629 words not in w2v model\n",
      "\n",
      "87280 words in w2v model\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "not_in_model = 0\n",
    "in_model = 0\n",
    "for word, i in word_index.items():\n",
    "    if word in w2v_model:\n",
    "        in_model += 1\n",
    "        embedding_matrix[i] = np.asarray(w2v_model[word], dtype='float32')\n",
    "    else:\n",
    "        not_in_model += 1\n",
    "print (str(not_in_model)+' words not in w2v model\\n')\n",
    "print (str(in_model)+' words in w2v model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train docs: 17597\n",
      "val docs: 2200\n",
      "test docs: 2200\n"
     ]
    }
   ],
   "source": [
    "p1 = int(len(data)*(1-VALIDATION_SPLIT-TEST_SPLIT))\n",
    "p2 = int(len(data)*(1-TEST_SPLIT))\n",
    "x_train = data[:p1]\n",
    "y_train = label[:p1]\n",
    "x_val = data[p1:p2]\n",
    "y_val = label[p1:p2]\n",
    "x_test = data[p2:]\n",
    "y_test = label[p2:]\n",
    "print ('train docs: '+str(len(x_train)))\n",
    "print ('val docs: '+str(len(x_val)))\n",
    "print ('test docs: '+str(len(x_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 100, 300)          64773000  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 200)               400800    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 402       \n",
      "=================================================================\n",
      "Total params: 65,174,202\n",
      "Trainable params: 401,202\n",
      "Non-trainable params: 64,773,000\n",
      "_________________________________________________________________\n",
      "['loss', 'acc']\n",
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 17597 samples, validate on 2200 samples\n",
      "Epoch 1/2\n",
      "17597/17597 [==============================] - 32s 2ms/step - loss: 0.0076 - acc: 0.9944 - val_loss: 1.1926e-07 - val_acc: 1.0000\n",
      "Epoch 2/2\n",
      "17597/17597 [==============================] - 31s 2ms/step - loss: 1.3238e-07 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "2200/2200 [==============================] - 1s 656us/step\n",
      "[1.1920928955078125e-07, 1.0]\n"
     ]
    }
   ],
   "source": [
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(LSTM(200, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(label.shape[1], activation='softmax'))\n",
    "model.summary()\n",
    "#plot_model(model, to_file='model.png',show_shapes=True)\n",
    " \n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "print (model.metrics_names)\n",
    "model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=2, batch_size=128)\n",
    "print (model.evaluate(x_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
